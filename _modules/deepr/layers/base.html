

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>deepr.layers.base &mdash; deepr 2.11.0 documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> deepr
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../getting_started/quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting_started/pipeline.html">Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting_started/config.html">Config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting_started/tuning.html">Hyper Parameter Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting_started/advanced.html">Advanced</a></li>
</ul>
<p class="caption"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../movielens/movielens.html">MovieLens Example</a></li>
</ul>
<p class="caption"><span class="caption-text">Developper documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../README.html">DeepR: Build and Train Deep Learning Pipelines for Production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../CONTRIBUTING.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../CHANGELOG.html">Changelog</a></li>
</ul>
<p class="caption"><span class="caption-text">API documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../API/core.html">DeepR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../_source/modules.html">deepr</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">deepr</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>deepr.layers.base</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for deepr.layers.base</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Interface for Layers&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Type</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="kn">from</span> <span class="nn">deepr.utils.datastruct</span> <span class="kn">import</span> <span class="n">item_to_dict</span><span class="p">,</span> <span class="n">dict_to_item</span><span class="p">,</span> <span class="n">to_flat_tuple</span>


<span class="n">LOGGER</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<div class="viewcode-block" id="Layer"><a class="viewcode-back" href="../../../_source/deepr.layers.html#deepr.layers.Layer">[docs]</a><span class="k">class</span> <span class="nc">Layer</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base class for composable layers in a deep learning network.</span>

<span class="sd">    Heavily inspired by TRAX layers, adapted for TF1.X and tf.estimator.</span>

<span class="sd">    Layers are the basic building block of models. A :class:`~Layer` is a</span>
<span class="sd">    function from one or more inputs to one or more outputs.</span>

<span class="sd">    The inputs of a :class:`~Layer` are tensors, packaged as follows</span>
<span class="sd">      - n_in = 1: one tensor (NOT wrapped in a tuple)</span>
<span class="sd">      - n_in &gt; 1: a tuple of tensors</span>

<span class="sd">    The outputs of a :class:`~Layer` are tensors, packaged as follows</span>
<span class="sd">      - n_out = 1: one tensor (NOT wrapped in a tuple)</span>
<span class="sd">      - n_out &gt; 1: a tuple of tensors</span>

<span class="sd">    The basic usage of a :class:`~Layer` is to build graphs as intuitively as</span>
<span class="sd">    possible. For example:</span>

<span class="sd">    &gt;&gt;&gt; from deepr.layers import Dense</span>
<span class="sd">    &gt;&gt;&gt; input_tensor = tf.ones([32, 8])</span>
<span class="sd">    &gt;&gt;&gt; dense = Dense(16)</span>
<span class="sd">    &gt;&gt;&gt; output_tensor = dense(input_tensor)</span>
<span class="sd">    &gt;&gt;&gt; output_tensor</span>
<span class="sd">    &lt;tf.Tensor &#39;dense/BiasAdd:0&#39; shape=(32, 16) dtype=float32&gt;</span>


<span class="sd">    Because some layers (like :class:`~Dropout`) might behave differently</span>
<span class="sd">    depending on the mode (TRAIN, EVAL, PREDICT), an optional argument</span>
<span class="sd">    can be provided:</span>

<span class="sd">    &gt;&gt;&gt; from deepr.layers import Dropout</span>
<span class="sd">    &gt;&gt;&gt; tensor = tf.ones([32, 8])</span>
<span class="sd">    &gt;&gt;&gt; dropout = Dropout(0.5)</span>
<span class="sd">    &gt;&gt;&gt; dropped = dropout(input_tensor, tf.estimator.ModeKeys.TRAIN)</span>
<span class="sd">    &gt;&gt;&gt; not_dropped = dropout(input_tensor, tf.estimator.ModeKeys.EVAL)</span>

<span class="sd">    Because in a lot of cases, a :class:`~Layer` needs to be applied on a</span>
<span class="sd">    dictionary, yielded by a tf.data.Dataset for example, you can also</span>
<span class="sd">    do:</span>

<span class="sd">    &gt;&gt;&gt; tf.reset_default_graph()</span>
<span class="sd">    &gt;&gt;&gt; tensors = {&quot;x&quot;: tf.ones([32, 8])}</span>
<span class="sd">    &gt;&gt;&gt; dense = Dense(16, inputs=&quot;x&quot;, outputs=&quot;y&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tensors = dense(tensors)</span>
<span class="sd">    &gt;&gt;&gt; tensors</span>
<span class="sd">    {&#39;y&#39;: &lt;tf.Tensor &#39;dense/BiasAdd:0&#39; shape=(32, 16) dtype=float32&gt;}</span>

<span class="sd">    The `inputs` and `outputs` are optional (defaults to t_0, t_1 etc.)</span>
<span class="sd">    and their order needs to be coherent with the order of tensors in</span>
<span class="sd">    tuples.</span>

<span class="sd">    Authors of new layer subclasses typically override one of the two</span>
<span class="sd">    methods of the base :class:`~Layer` class::</span>

<span class="sd">        def forward(self, tensors, mode: str = None):</span>
<span class="sd">            # tensors is either a Tensor (n_in=1) or a tuple of Tensors</span>

<span class="sd">        def forward_as_dict(self, tensors: Dict, mode: str = None) -&gt; Dict:</span>
<span class="sd">            # tensors is a dictionary whose keys contain self.inputs</span>

<span class="sd">    The implementation of either of these two methods gives the</span>
<span class="sd">    implementation of the other for free thanks to automatic tuple to</span>
<span class="sd">    dictionary conversion.</span>

<span class="sd">    The easiest way to define custom layers is to use the :class:`~layer`</span>
<span class="sd">    decorator (see documentation).</span>

<span class="sd">    Note that layers using parameters (a :class:`~Dense` layer for example)</span>
<span class="sd">    should not create variables at instantiation time nor store</span>
<span class="sd">    variables or any other graph references as attributes.</span>

<span class="sd">    &gt;&gt;&gt; tf.reset_default_graph()</span>
<span class="sd">    &gt;&gt;&gt; dense = Dense(16)</span>

<span class="sd">    No parameters are created</span>
<span class="sd">    &gt;&gt;&gt; dense(tf.ones([32, 8]))</span>
<span class="sd">    &lt;tf.Tensor &#39;dense/BiasAdd:0&#39; shape=(32, 16) dtype=float32&gt;</span>

<span class="sd">    Parameters are created in the current tf.Graph</span>

<span class="sd">    In other words, calling the layer should not change its state. This</span>
<span class="sd">    is effectively enforcing functional programming. The state of the</span>
<span class="sd">    layer is only used to parametrize its runtime. This makes it simpler</span>
<span class="sd">    to define graphs with the tf.estimator API.</span>

<span class="sd">    If you want to define a layer and use it twice (effectively reusing</span>
<span class="sd">    its variables), you need to be explicit, and set the `reuse=True`</span>
<span class="sd">    arguments at call time. Behind the scene, it&#39;s simply wrapping the</span>
<span class="sd">    TF1.X variable management into a :meth:`~tf.variable_scope`.</span>

<span class="sd">    &gt;&gt;&gt; tf.reset_default_graph()</span>
<span class="sd">    &gt;&gt;&gt; dense = Dense(16)</span>
<span class="sd">    &gt;&gt;&gt; dense(tf.ones([32, 8]))</span>
<span class="sd">    &lt;tf.Tensor &#39;dense/BiasAdd:0&#39; shape=(32, 16) dtype=float32&gt;</span>
<span class="sd">    &gt;&gt;&gt; dense(tf.ones([32, 8]), reuse=True)</span>
<span class="sd">    &lt;tf.Tensor &#39;dense_1/BiasAdd:0&#39; shape=(32, 16) dtype=float32&gt;</span>

<span class="sd">    While the two operations have different names &#39;dense/BiasAdd:0&#39; and</span>
<span class="sd">    &#39;dense_1/BiasAdd:0&#39;, they both share the same weights.</span>

<span class="sd">    Good examples on how to implement parametrized layers are deepr.Dense</span>
<span class="sd">    and embedding.Embedding.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    n_in : int</span>
<span class="sd">        Number of expected inputs, &gt;= 1</span>
<span class="sd">    n_out : int</span>
<span class="sd">        Number of expected outputs, &gt;= 1</span>
<span class="sd">    inputs : Union[str, Tuple[str, ...]], Optional</span>
<span class="sd">        Names of the n_in inputs keys in a dictionary.</span>
<span class="sd">        Tuple if n_in &gt; 1, else string.</span>
<span class="sd">    outputs : Union[str, Tuple[str, ...]], Optional</span>
<span class="sd">        Names of the n_out outputs keys in a dictionary.</span>
<span class="sd">        Tuple if n_out &gt; 1, else string</span>
<span class="sd">    name : str, optional</span>
<span class="sd">        Name of the layer</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="Layer.__init__"><a class="viewcode-back" href="../../../API/_autosummary/deepr.layers.Layer.html#deepr.layers.Layer.__init__">[docs]</a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_in</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># Assert either number of inputs or names of inputs are given</span>
        <span class="k">if</span> <span class="n">n_in</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must set either n_in or inputs (both are None)&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">n_out</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must set either n_out or outputs (both are None)&quot;</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">_default_names</span><span class="p">(</span><span class="n">num</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;t_&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">num</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">0&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}{</span><span class="n">idx</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num</span><span class="p">))</span>

        <span class="c1"># Resolve n_in / inputs from arguments</span>
        <span class="k">if</span> <span class="n">n_in</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">n_in</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">to_flat_tuple</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">n_in</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">_default_names</span><span class="p">(</span><span class="n">n_in</span><span class="p">)</span>

        <span class="c1"># Resolve n_out / outputs from arguments</span>
        <span class="k">if</span> <span class="n">n_out</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">outputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">n_out</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">to_flat_tuple</span><span class="p">(</span><span class="n">outputs</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">n_out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">_default_names</span><span class="p">(</span><span class="n">n_out</span><span class="p">)</span>

        <span class="c1"># For mypy</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">n_out</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>

        <span class="c1"># Store attributes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_in</span> <span class="o">=</span> <span class="n">n_in</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_out</span> <span class="o">=</span> <span class="n">n_out</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>

        <span class="c1"># Assert coherent attributes</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_in</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Layer </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2"> inputs should be a string (n_in = 1)&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_out</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Layer </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2"> outputs should be a string (n_out = 1)&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">to_flat_tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">))</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_in</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2">: `inputs` inconsistent with `n_in` (n_in=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">n_in</span><span class="si">}</span><span class="s2">, inputs=&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="si">}</span><span class="s2">)&#39;&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">to_flat_tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">))</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_out</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2">: `outputs` inconsistent with `n_out` (n_out=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">n_out</span><span class="si">}</span><span class="s2">, outputs=&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="si">}</span><span class="s2">&#39;&#39;)&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">args</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">n_in</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">n_out</span><span class="si">}</span><span class="s2">, inputs=&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="si">}</span><span class="s2">&#39;, outputs=&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="si">}</span><span class="s2">&#39;, name=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(</span><span class="si">{</span><span class="n">args</span><span class="si">}</span><span class="s2">)&quot;</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">tensors</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]],</span>
        <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">reuse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;Forward as tuple or dictionary depending on tensors type.</span>

<span class="sd">        Wraps the layer call in a variable scope to be able to reuse</span>
<span class="sd">        variable with the ``reuse`` argument, adds a tf.identity</span>
<span class="sd">        operator to each output tensor using self.outputs.</span>

<span class="sd">        If tensors is a Dict, it returns a dictionary whose keys are</span>
<span class="sd">        defined by self.outputs.</span>

<span class="sd">        Otherwise, input tensors type is expected to be, if</span>
<span class="sd">            - n_in = 1: one tensor (NOT wrapped in a tuple)</span>
<span class="sd">            - n_in &gt; 1: a tuple of tensors</span>
<span class="sd">        In that case, output tensors type is expected to be, if</span>
<span class="sd">            - n_out = 1: one tensor (NOT wrapped in a tuple)</span>
<span class="sd">            - n_out &gt; 1: a tuple of tensors</span>

<span class="sd">        NOTE: Each call to this method performs inspection on the inputs</span>
<span class="sd">        and outputs type, which can be costly in terms of computation.</span>

<span class="sd">        This is not an issue when building graphs with tf.estimator as</span>
<span class="sd">        the graph is compiled once and for all.</span>

<span class="sd">        However, when using a :class:`~Layer` to preprocess a :class:`~tf.data.Dataset`</span>
<span class="sd">        (eg. with a ``map`` transformation), this method will be called</span>
<span class="sd">        for each example and might cause slowdown. It is recommended to</span>
<span class="sd">        explicitly use ``forward`` or ``forward_as_dict`` in that case.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        tensors : Union[tf.Tensor, Dict[str, tf.Tensor], Tuple[tf.Tensor, ...]]</span>
<span class="sd">            Input tensors</span>
<span class="sd">        mode : str, optional</span>
<span class="sd">            One of tf.estimator.ModeKeys</span>
<span class="sd">        reuse : bool, optional</span>
<span class="sd">            Encapsulates layer call in a variable scope with reuse=reuse</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">get_variable_scope</span><span class="p">(),</span> <span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="c1"># Check that tensors is coherent with self.inputs</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">set</span><span class="p">(</span><span class="n">to_flat_tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">))</span> <span class="o">&lt;=</span> <span class="nb">set</span><span class="p">(</span><span class="n">tensors</span><span class="p">):</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2"> missing inputs: </span><span class="si">{</span><span class="nb">set</span><span class="p">(</span><span class="n">to_flat_tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">))</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

                <span class="c1"># Call forward_as_dict to get output tensors</span>
                <span class="n">tensors_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_as_dict</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">mode</span><span class="p">)</span>

                <span class="c1"># Check that tensors_dict is coherent with self.outputs</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">set</span><span class="p">(</span><span class="n">to_flat_tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">))</span> <span class="o">&lt;=</span> <span class="nb">set</span><span class="p">(</span><span class="n">tensors_dict</span><span class="p">):</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2"> missing outputs: </span><span class="si">{</span><span class="nb">set</span><span class="p">(</span><span class="n">to_flat_tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">))</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">tensors_dict</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

                <span class="k">return</span> <span class="n">tensors_dict</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Check that tensors is coherent with self.n_in</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_in</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2"> expected 1 input, but got </span><span class="si">{</span><span class="n">tensors</span><span class="si">}</span><span class="s2"> (should not be a tuple)&quot;</span>
                    <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_in</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">to_flat_tuple</span><span class="p">(</span><span class="n">tensors</span><span class="p">))</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_in</span><span class="p">:</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2"> expected </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">n_in</span><span class="si">}</span><span class="s2"> inputs, but got </span><span class="si">{</span><span class="n">tensors</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

                <span class="c1"># Call forward and convert to tuple</span>
                <span class="n">tensors_tuple</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">mode</span><span class="p">)</span>

                <span class="c1"># Check that tensors_tuple is coherent with outputs</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">to_flat_tuple</span><span class="p">(</span><span class="n">tensors_tuple</span><span class="p">))</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_out</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">n_out</span><span class="si">}</span><span class="s2"> outputs but got </span><span class="si">{</span><span class="n">tensors_tuple</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

                <span class="k">return</span> <span class="n">tensors_tuple</span>

<div class="viewcode-block" id="Layer.forward"><a class="viewcode-back" href="../../../_source/deepr.layers.html#deepr.layers.Layer.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">tensors</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]],</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;Forward method on one Tensor or a tuple of Tensors.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        tensors : Union[tf.Tensor, Tuple[tf.Tensor, ...]]</span>
<span class="sd">            - n_in = 1: one tensor (NOT wrapped in a tuple)</span>
<span class="sd">            - n_in &gt; 1: a tuple of tensors</span>
<span class="sd">        mode : str, optional</span>
<span class="sd">            Description</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Union[tf.Tensor, Tuple[tf.Tensor, ...]]</span>
<span class="sd">            - n_out = 1: one tensor (NOT wrapped in a tuple)</span>
<span class="sd">            - n_out &gt; 1: a tuple of tensors</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">dict_to_item</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forward_as_dict</span><span class="p">(</span><span class="n">item_to_dict</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">),</span> <span class="n">mode</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">LOGGER</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2"> error on </span><span class="si">{</span><span class="n">tensors</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">raise</span> <span class="n">e</span></div>

<div class="viewcode-block" id="Layer.forward_as_dict"><a class="viewcode-back" href="../../../_source/deepr.layers.html#deepr.layers.Layer.forward_as_dict">[docs]</a>    <span class="k">def</span> <span class="nf">forward_as_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensors</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Forward method on a dictionary of Tensors.</span>

<span class="sd">        The input ``tensors`` should contain all keys defined in</span>
<span class="sd">        ``self.inputs`` (but might contain more keys).</span>
<span class="sd">        It returns a new dictionary (does not mutate the input</span>
<span class="sd">        ``tensors`` dictionary in-place), whose keys are exactly</span>
<span class="sd">        ``self.outputs``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        tensors : Dict[str, tf.Tensor]</span>
<span class="sd">            Dictionary mapping self.inputs to tf.Tensors.</span>
<span class="sd">        mode : str, optional</span>
<span class="sd">            One of tf.estimator.ModeKeys</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Dict[str, tf.Tensor]</span>
<span class="sd">            Dictionary mapping self.outputs to tf.Tensors</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">item_to_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">dict_to_item</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">),</span> <span class="n">mode</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">LOGGER</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2"> error on </span><span class="si">{</span><span class="n">tensors</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">raise</span> <span class="n">e</span></div></div>


<div class="viewcode-block" id="Lambda"><a class="viewcode-back" href="../../../_source/deepr.layers.html#deepr.layers.Lambda">[docs]</a><span class="k">class</span> <span class="nc">Lambda</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Lambda layer.</span>

<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    &gt;&gt;&gt; from deepr.layers import Lambda</span>
<span class="sd">    &gt;&gt;&gt; add_one = Lambda(lambda tensors, _: tensors + 1, inputs=&quot;x&quot;, outputs=&quot;y&quot;)</span>
<span class="sd">    &gt;&gt;&gt; add_one(1)</span>
<span class="sd">    2</span>
<span class="sd">    &gt;&gt;&gt; add_one({&quot;x&quot;: 1})</span>
<span class="sd">    {&#39;y&#39;: 2}</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fn</span> <span class="o">=</span> <span class="n">fn</span>

<div class="viewcode-block" id="Lambda.forward"><a class="viewcode-back" href="../../../_source/deepr.layers.html#deepr.layers.Lambda.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">tensors</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]],</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fn</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">mode</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="layer"><a class="viewcode-back" href="../../../_source/deepr.layers.html#deepr.layers.layer">[docs]</a><span class="k">def</span> <span class="nf">layer</span><span class="p">(</span>
    <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_in</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Decorator that creates a layer constructor from a function.</span>

<span class="sd">    The decorator returns a subclass of :class:`~Layer`</span>
<span class="sd">    whose ``forward`` method is defined by the decorated function.</span>

<span class="sd">    For example</span>

<span class="sd">    &gt;&gt;&gt; from deepr.layers import layer</span>
<span class="sd">    &gt;&gt;&gt; @layer(n_in=1, n_out=1)</span>
<span class="sd">    ... def AddOffset(tensors, mode, offset):</span>
<span class="sd">    ...     return tensors + offset</span>
<span class="sd">    &gt;&gt;&gt; add = AddOffset(offset=1)</span>
<span class="sd">    &gt;&gt;&gt; add(1)</span>
<span class="sd">    2</span>

<span class="sd">    The class created by the decorator is roughly equivalent to</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        class AddOffset(Layer):</span>

<span class="sd">            def __init__(self, offset, n_in=1, n_out=1, inputs=None, outputs=None, name=None):</span>
<span class="sd">                Layer.__init__(n_in=n_in, n_out=n_out, inputs=inputs, outputs=outputs, name=name)</span>
<span class="sd">                self.offset = offset</span>

<span class="sd">            def forward(self, tensors, mode: str = None):</span>
<span class="sd">                return tensors + self.offset</span>

<span class="sd">    You can also add a &#39;mode&#39; argument to your layer like so</span>
<span class="sd">    &gt;&gt;&gt; @layer(n_in=1, n_out=1)</span>
<span class="sd">    ... def AddOffsetInTrain(tensors, mode, offset):</span>
<span class="sd">    ...     if mode == tf.estimator.ModeKeys.TRAIN:</span>
<span class="sd">    ...         return tensors + offset</span>
<span class="sd">    ...     else:</span>
<span class="sd">    ...         return tensors</span>
<span class="sd">    &gt;&gt;&gt; add = AddOffsetInTrain(offset=1)</span>
<span class="sd">    &gt;&gt;&gt; add(1, tf.estimator.ModeKeys.TRAIN)</span>
<span class="sd">    2</span>
<span class="sd">    &gt;&gt;&gt; add(1, tf.estimator.ModeKeys.PREDICT)</span>
<span class="sd">    1</span>

<span class="sd">    Note that &#39;tensors&#39; and &#39;mode&#39; need to be the the first arguments</span>
<span class="sd">    of the function IN THIS ORDER.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># pylint: disable=protected-access,invalid-name</span>
    <span class="k">def</span> <span class="nf">_create_layer_class</span><span class="p">(</span><span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Type</span><span class="p">[</span><span class="n">Layer</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Decorator that creates a Layer constructor.&quot;&quot;&quot;</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span>
        <span class="n">signature</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">Signature</span><span class="p">([</span><span class="n">param</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">&quot;tensors&quot;</span><span class="p">,</span> <span class="s2">&quot;mode&quot;</span><span class="p">}])</span>

        <span class="c1"># Check parameters</span>
        <span class="k">if</span> <span class="nb">list</span><span class="p">(</span><span class="n">parameters</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">&quot;tensors&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&#39;tensors&#39; should be the first parameter of </span><span class="si">{</span><span class="n">fn</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;mode&quot;</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">list</span><span class="p">(</span><span class="n">parameters</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">&quot;mode&quot;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&#39;mode&#39; should be the second parameter of </span><span class="si">{</span><span class="n">fn</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="n">_n_in</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;n_in&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="s2">&quot;n_in&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="k">else</span> <span class="n">n_in</span>
            <span class="n">_n_out</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;n_out&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="s2">&quot;n_out&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="k">else</span> <span class="n">n_out</span>
            <span class="n">_inputs</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;inputs&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="s2">&quot;inputs&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="k">else</span> <span class="n">inputs</span>
            <span class="n">_outputs</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;outputs&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="s2">&quot;outputs&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="k">else</span> <span class="n">outputs</span>
            <span class="n">_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="s2">&quot;name&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="n">Layer</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_in</span><span class="o">=</span><span class="n">_n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="o">=</span><span class="n">_n_out</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">_outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">_name</span><span class="p">)</span>
            <span class="n">signature</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_args</span> <span class="o">=</span> <span class="n">args</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>

        <span class="k">if</span> <span class="s2">&quot;mode&quot;</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>

            <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensors</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_args</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_kwargs</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>

            <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensors</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
                <span class="c1"># pylint: disable=unused-argument</span>
                <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_args</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_kwargs</span><span class="p">)</span>

        <span class="n">attributes</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;__module__&quot;</span><span class="p">:</span> <span class="n">fn</span><span class="o">.</span><span class="vm">__module__</span><span class="p">,</span> <span class="s2">&quot;__doc__&quot;</span><span class="p">:</span> <span class="n">fn</span><span class="o">.</span><span class="vm">__doc__</span><span class="p">,</span> <span class="s2">&quot;__init__&quot;</span><span class="p">:</span> <span class="n">_init</span><span class="p">,</span> <span class="s2">&quot;forward&quot;</span><span class="p">:</span> <span class="n">_forward</span><span class="p">}</span>
        <span class="k">return</span> <span class="nb">type</span><span class="p">(</span><span class="n">fn</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="p">(</span><span class="n">Layer</span><span class="p">,),</span> <span class="n">attributes</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_create_layer_class</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_create_layer_class</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, Criteo.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>