{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepr faiss_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-08-03 15:35:03--  http://files.grouplens.org/datasets/movielens/ml-20m.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 198702078 (189M) [application/zip]\n",
      "Saving to: 'ml-20m.zip'\n",
      "\n",
      "100%[======================================>] 198,702,078 26.2MB/s   in 7.9s   \n",
      "\n",
      "2020-08-03 15:35:12 (24.0 MB/s) - 'ml-20m.zip' saved [198702078/198702078]\n",
      "\n",
      "Archive:  ml-20m.zip\n",
      "   creating: ml-20m/\n",
      "  inflating: ml-20m/genome-scores.csv  \n",
      "  inflating: ml-20m/genome-tags.csv  \n",
      "  inflating: ml-20m/links.csv        \n",
      "  inflating: ml-20m/movies.csv       \n",
      "  inflating: ml-20m/ratings.csv      \n",
      "  inflating: ml-20m/README.txt       \n",
      "  inflating: ml-20m/tags.csv         \n"
     ]
    }
   ],
   "source": [
    "!wget http://files.grouplens.org/datasets/movielens/ml-20m.zip\n",
    "!unzip ml-20m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dataset_path=os.getcwd()+\"/ml-20m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import tensorflow as tf\n",
    "from deepr.examples import movielens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ratings = f\"{dataset_path}/ratings.csv\"\n",
    "\n",
    "path_root = \"wan\"\n",
    "path_model = path_root + \"/model\"\n",
    "path_data = path_root + \"/data\"\n",
    "path_variables = path_root + \"/variables\"\n",
    "path_predictions = path_root + \"/predictions.parquet.snappy\"\n",
    "path_saved_model = path_root + \"/saved_model\"\n",
    "path_mapping = path_data + \"/mapping.txt\"\n",
    "path_train = path_data + \"/train.tfrecord.gz\"\n",
    "path_eval = path_data + \"/eval.tfrecord.gz\"\n",
    "path_test = path_data + \"/test.tfrecord.gz\"\n",
    "dpr.io.Path(path_root).mkdir(exist_ok=True)\n",
    "dpr.io.Path(path_model).mkdir(exist_ok=True)\n",
    "dpr.io.Path(path_data).mkdir(exist_ok=True)\n",
    "max_steps = 100_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we first download the movielens dataset then do a training using deepr library\n",
    "It takes 30min to get movie embeddings and a timeline embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build TF record\n",
    "\n",
    "The job takes as input the csv ratings and create 3 tfrecords files (train, validation, test). Each file contains timeline of user ratings split into input and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build TF Records before defining the train job (need vocab size)\n",
    "build = movielens.jobs.BuildRecords(\n",
    "    path_ratings=path_ratings,\n",
    "    path_mapping=path_mapping,\n",
    "    path_train=path_train,\n",
    "    path_eval=path_eval,\n",
    "    path_test=path_test,\n",
    "    min_rating=4,\n",
    "    min_length=5,\n",
    "    num_negatives=8,\n",
    "    target_ratio=0.2,\n",
    "    size_test=10_000,\n",
    "    size_eval=10_000,\n",
    "    shuffle_timelines=True,\n",
    "    seed=2020,\n",
    ")\n",
    "build.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "This defines a training job to build a model that transform a timeline of movies embedding to an user embedding.\n",
    "It takes as input the tf records and upon completion writes 3 artifacts :\n",
    "* dataframe of the biases and the embeddings\n",
    "* saved model : protobuf containing the model definition and weights\n",
    "\n",
    "In this specific instance we train an average model with a BPR loss and compute a triple precision on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train, predict and evaluate jobs\n",
    "train = dpr.jobs.Trainer(\n",
    "    path_model=path_model,\n",
    "    pred_fn=movielens.layers.AverageModel(vocab_size=dpr.vocab.size(path_mapping), dim=100),\n",
    "    loss_fn=movielens.layers.BPRLoss(vocab_size=dpr.vocab.size(path_mapping), dim=100),\n",
    "    optimizer_fn=dpr.optimizers.TensorflowOptimizer(\"LazyAdam\", 0.001),\n",
    "    train_input_fn=dpr.readers.TFRecordReader(path_train),\n",
    "    eval_input_fn=dpr.readers.TFRecordReader(path_eval, shuffle=False),\n",
    "    prepro_fn=movielens.prepros.RecordPrepro(\n",
    "        min_input_size=3,\n",
    "        min_target_size=3,\n",
    "        max_input_size=50,\n",
    "        max_target_size=50,\n",
    "        buffer_size=1024,\n",
    "        batch_size=128,\n",
    "        repeat_size=None,\n",
    "        prefetch_size=1,\n",
    "        num_parallel_calls=8,\n",
    "    ),\n",
    "    train_spec=dpr.jobs.TrainSpec(max_steps=max_steps),\n",
    "    eval_spec=dpr.jobs.EvalSpec(steps=None, start_delay_secs=30, throttle_secs=30),\n",
    "    final_spec=dpr.jobs.FinalSpec(steps=None),\n",
    "    exporters=[\n",
    "        # the training will keep the model with the best triplet precision\n",
    "        dpr.exporters.BestCheckpoint(metric=\"triplet_precision\", mode=\"increase\"),\n",
    "        # export biases and embeddings as a dataframe\n",
    "        dpr.exporters.SaveVariables(path_variables=path_variables, variable_names=[\"biases\", \"embeddings\"]),\n",
    "        # export a saved model using specified fields as input\n",
    "        dpr.exporters.SavedModel(\n",
    "            path_saved_model=path_saved_model,\n",
    "            fields=[\n",
    "                dpr.Field(name=\"inputPositives\", shape=(None,), dtype=tf.int64),\n",
    "                dpr.Field(name=\"inputMask\", shape=(None,), dtype=tf.bool),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    train_hooks=[\n",
    "        # log metrics, hyperparams, initial values to the console, and optionally mlflow and graphite\n",
    "        dpr.hooks.LoggingTensorHookFactory(\n",
    "            name=\"training\",\n",
    "            functions={\n",
    "                \"memory_gb\": dpr.hooks.ResidentMemory(unit=\"gb\"),\n",
    "                \"max_memory_gb\": dpr.hooks.MaxResidentMemory(unit=\"gb\"),\n",
    "            },\n",
    "            every_n_iter=300,\n",
    "            use_graphite=False,\n",
    "            use_mlflow=False,\n",
    "        ),\n",
    "        dpr.hooks.SummarySaverHookFactory(save_steps=300),\n",
    "        dpr.hooks.NumParamsHook(use_mlflow=False),\n",
    "        dpr.hooks.LogVariablesInitHook(use_mlflow=False),\n",
    "        dpr.hooks.StepsPerSecHook(\n",
    "            name=\"training\",\n",
    "            batch_size=128,\n",
    "            every_n_steps=300,\n",
    "            skip_after_step=max_steps,\n",
    "            use_mlflow=False,\n",
    "            use_graphite=False,\n",
    "        ),\n",
    "        # stop the training if triplet precision does not improve\n",
    "        dpr.hooks.EarlyStoppingHookFactory(\n",
    "            metric=\"triplet_precision\",\n",
    "            mode=\"increase\",\n",
    "            max_steps_without_improvement=1000,\n",
    "            min_steps=5_000,\n",
    "            run_every_steps=300,\n",
    "            final_step=max_steps,\n",
    "        ),\n",
    "    ],\n",
    "    eval_hooks=[dpr.hooks.LoggingTensorHookFactory(name=\"validation\", at_end=True)],\n",
    "    final_hooks=[dpr.hooks.LoggingTensorHookFactory(name=\"final_validation\", at_end=True)],\n",
    "    train_metrics=[dpr.metrics.StepCounter(name=\"num_steps\"), dpr.metrics.DecayMean(tensors=[\"loss\"], decay=0.98)],\n",
    "    eval_metrics=[dpr.metrics.Mean(tensors=[\"loss\", \"triplet_precision\"])],\n",
    "    final_metrics=[dpr.metrics.Mean(tensors=[\"loss\", \"triplet_precision\"])],\n",
    "    run_config=dpr.jobs.RunConfig(\n",
    "        save_checkpoints_steps=300, save_summary_steps=300, keep_checkpoint_max=None, log_step_count_steps=300\n",
    "    ),\n",
    "    config_proto=dpr.jobs.ConfigProto(\n",
    "        inter_op_parallelism_threads=8, intra_op_parallelism_threads=8, gpu_device_count=0, cpu_device_count=48,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict job\n",
    "The predict job reloads the test dataset and the saved model, perform the inference and write the user embeddings in a dataframe to be reused later by the validation job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = movielens.jobs.Predict(\n",
    "    path_saved_model=path_saved_model,\n",
    "    path_predictions=path_predictions,\n",
    "    input_fn=dpr.readers.TFRecordReader(path_test, shuffle=False),\n",
    "    prepro_fn=movielens.prepros.RecordPrepro(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation job\n",
    "This job takes user embedding, and uses faiss to retrieve the k nearest neighboors in the product embedding space, \n",
    "and compute metrics using the target timelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the validation metrics\n",
    "evaluate = [\n",
    "    movielens.jobs.Evaluate(\n",
    "        path_predictions=path_predictions,\n",
    "        path_embeddings=path_variables + \"/embeddings\",\n",
    "        path_biases=path_variables + \"/biases\",\n",
    "        k=k,\n",
    "    )\n",
    "    for k in [10, 20, 50]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "All the jobs definition are lazy, and so is the pipeline. \n",
    "Calling run on it will actually perform all these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline\n",
    "pipeline = dpr.jobs.Pipeline([train, predict] + evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Search\n",
    "\n",
    "Let's check if the movie embeddings produce make sense\n",
    "* load the movie embeddings\n",
    "* load the movie title\n",
    "* build a knn index on that\n",
    "* do a query with a known movie to check that its closest neighboors make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import faiss\n",
    "import pyarrow.csv as pc\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "embeddings = np.vstack(pq.read_table(\"wan/variables/embeddings\").to_pandas().to_numpy())\n",
    "mapp = {int(movie_id):indice for indice, movie_id in enumerate(open(\"wan/data/mapping.txt\", \"r\").read().split(\"\\n\"))}\n",
    "inversed_map = {indice:movie_id for movie_id, indice in mapp.items()}\n",
    "index = faiss.IndexFlatIP(embeddings.shape[-1])\n",
    "index.add(np.ascontiguousarray(embeddings))\n",
    "\n",
    "def knn_query(index, query, ksearch):\n",
    "    D, I = index.search(np.expand_dims(query,0), ksearch)\n",
    "    distances = D[0]\n",
    "    product_indices = I[0]\n",
    "    product_ids = [inversed_map[i] for i in product_indices]\n",
    "    return list(zip(product_ids, distances))\n",
    "movies = pc.read_csv(f\"{dataset_path}/movies.csv\").to_pandas()\n",
    "    \n",
    "def display_results_df(results):\n",
    "    data = [[movies[movies.movieId == movie_id][\"genres\"].to_numpy()[0], movies[movies.movieId == movie_id][\"title\"].to_numpy()[0], distance] for movie_id, distance in results]\n",
    "    \n",
    "    df = pd.DataFrame(data, columns = ['Genre', 'Title', 'Distance']) \n",
    "    display(df)\n",
    "    \n",
    "def display_movie(movie_id):\n",
    "    data = [[(movies[movies.movieId == movie_id][\"genres\"].to_numpy()[0]), (movies[movies.movieId == movie_id][\"title\"].to_numpy()[0])]]\n",
    "    \n",
    "    df = pd.DataFrame(data, columns = ['Genre', 'Title']) \n",
    "    display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Action|Adventure|Sci-Fi</td>\n",
       "      <td>Star Wars: Episode IV - A New Hope (1977)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Genre                                      Title\n",
       "0  Action|Adventure|Sci-Fi  Star Wars: Episode IV - A New Hope (1977)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knn results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>Title</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Action|Adventure|Sci-Fi</td>\n",
       "      <td>Star Wars: Episode IV - A New Hope (1977)</td>\n",
       "      <td>24.185650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Action|Adventure|Sci-Fi</td>\n",
       "      <td>Star Wars: Episode VI - Return of the Jedi (1983)</td>\n",
       "      <td>21.296133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Action|Adventure|Sci-Fi</td>\n",
       "      <td>Star Wars: Episode V - The Empire Strikes Back...</td>\n",
       "      <td>20.707195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Action|Adventure</td>\n",
       "      <td>Raiders of the Lost Ark (Indiana Jones and the...</td>\n",
       "      <td>18.069126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Action|Adventure|Sci-Fi|Thriller</td>\n",
       "      <td>Star Trek: First Contact (1996)</td>\n",
       "      <td>16.387402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Genre  \\\n",
       "0           Action|Adventure|Sci-Fi   \n",
       "1           Action|Adventure|Sci-Fi   \n",
       "2           Action|Adventure|Sci-Fi   \n",
       "3                  Action|Adventure   \n",
       "4  Action|Adventure|Sci-Fi|Thriller   \n",
       "\n",
       "                                               Title   Distance  \n",
       "0          Star Wars: Episode IV - A New Hope (1977)  24.185650  \n",
       "1  Star Wars: Episode VI - Return of the Jedi (1983)  21.296133  \n",
       "2  Star Wars: Episode V - The Empire Strikes Back...  20.707195  \n",
       "3  Raiders of the Lost Ark (Indiana Jones and the...  18.069126  \n",
       "4                    Star Trek: First Contact (1996)  16.387402  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = movies[movies.title.str.lower().str.contains(\"star wars\")][\"movieId\"].to_numpy()[0]\n",
    "print(\"Query\")\n",
    "display_movie(p)\n",
    "print(\"Knn results\")\n",
    "display_results_df(knn_query(index, embeddings[mapp[p]], 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the genre of the nearest neighboors is similar and the movies are related to the movie query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
